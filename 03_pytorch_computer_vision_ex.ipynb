{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/pytorch/blob/main/03_pytorch_computer_vision_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ],
      "metadata": {
        "id": "Vex99np2wFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "06cfe753-3857-459e-e596-76dadeacba45"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  7 20:54:12 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    28W /  70W |   6600MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "3c0f001e-f976-4964-8dbf-23673cd11c92"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ],
      "metadata": {
        "id": "lvf-3pODxXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# download MNIST training and test data\n",
        "train_data = MNIST(root='data', train=True, download=True, transform=ToTensor(), target_transform=None)\n",
        "test_data = MNIST(root='data', train=False, download=True, transform=ToTensor(), target_transform=None)"
      ],
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ],
      "metadata": {
        "id": "qxZW-uAbxe_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "N_SAMPLES = 5\n",
        "# select 5 random indexes from the training data\n",
        "random_idx = random.sample(range(len(train_data)), k=N_SAMPLES)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 3))\n",
        "# plot the image and label from the training data at each random index from the list above\n",
        "for i, rand_idx in enumerate(random_idx):\n",
        "  image, label = train_data[rand_idx][0].squeeze(dim=0), train_data[rand_idx][1]\n",
        "  fig.add_subplot(1, 5, i+1)\n",
        "  plt.imshow(image, cmap='gray')\n",
        "  plt.title(f\"Class: {label}\")\n",
        "  plt.axis(False)\n",
        "\n"
      ],
      "metadata": {
        "id": "QVFsYi1PbItE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "593d861f-69bb-4b50-ca3b-92d6043e9a22"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x216 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAACuCAYAAADTXFfGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVoklEQVR4nO3dfZBcZb0n8N9DggloiuXKRaMYthAiChospMyl5CXgG3i94EtBQQpT7lIbwYsRE2HBTSaBdYVZTCHlUigluCoXIsXqbsDIrUtewXuRFbLRYEgBCwXspuQ9BI0J4ewfM2xhnqdDz+mZ7jkzn09VKtS3+/TzTOpHd//m9Pl1qqoqAAAAGJq9er0BAACAJtJMAQAA1KCZAgAAqEEzBQAAUINmCgAAoAbNFAAAQA2aqYhIKS1OKf2k1/uAdqlZmkbN0jRqlqZRs70xbpqplNLZKaX/mVLallL6vymlFSmlj/R6X69JKc1LKf3vlNLLKaXfp5Sm93pP9NZor9mIiJTSCSmlKqX0H3u9F3pvNNdsSunylNJvU0qvpJQW93o/jA5qlqYZ5TV7VEppXUrpxZTSkymlhb3eUzeMi2YqpfS1iLg6Iv5TRLwtIqZFxLURcVov9/WalNK5EfFvI+JTEfGWiPjbiHimp5uip0Z7zUZEpJT2jojvRMS9vd4LvdeAmn04Ii6KiDt6vRFGBzVL0zSgZv8hItZGxF9FxAkRcX5K6e96u6WRN+abqZTSfhFxWUR8uaqq/1ZV1ctVVe2sqmp5VVVfb3HMrSmlLYOd9dqU0hGvu+3UlNKDKaWXUkpPpZQWDOYHpJRuTym9kFJ6brAzf8N/38H79EXEhVVVPVgNeKSqqueG51+AphntNfs68yPiHyNiUwc/LmNAE2q2qqr/WlXVioh4aRh+ZBpOzdI0TajZiPjXEXFTVVW7qqp6JCLujogj9nxI8435Zioi/iYiJkfEz4ZwzIqIOCwiDoyI+yPiptfd9oOImFtV1ZSIODIiVg7m8yPiyYj46xj4bcGlEVFFRKSUrk0pXdtirYMG/xyZUnoiDXzUb8kQ39Qytoz2mo2U0sER8W9i4IkdRn3Nwm7ULE3ThJq9OiK+kFLaO6X0nsE9/9MQ9ttIE3u9gS54a0Q8U1XVK+0eUFXVDa/99+DnlJ9PKe1XVdWLEbEzIt6XUvpfVVU9HxHPD951Z0RMjYiDq6p6OCLWve7xzt/DcgcN/v3xiHh/RPyrGPht/5MRcX27e2ZMGe01GxFxTUQsrKpqW0qp3W0ydjWhZuH11CxN04SavT0ifhQRCyJiQkRcVlXVfe3ut6nGw9mPZyPigJRSW41jSmlCSumKlNIjKaWtEfHY4E0HDP79uYg4NSIeTymtSSn9zWD+n2Pg883/mFJ6NKX079vc358G/+6vquqFqqoei4jvDa7B+DSqazal9OmImFJV1bI2fx7GvlFds1CgZmmaUV2zKaW/iohfxsAnViZHxLsi4hMppTH/S4Px0Ez9c0T8OSJOb/P+Z8fAhXwfjYj9YuDznxERKSKiqqr7qqo6LQZOmf48In46mL9UVdX8qqoOiYi/i4ivpZRObmO9hyJiRwyeQh1Utbgv48Nor9mTI+JDg5/D3hIRZ0bEV1NK/73N/TL2jPaahd2pWZpmtNfsIRGxq6qqH1VV9UpVVU9GxC0xDk4OjPlmavBU5qKI+C8ppdNTSvsOfpbzlJRSf+GQKTFQrM9GxL4xMDElIiJSSm9KKc0ePEW6MyK2RsSrg7f9bUrp0DTwmacXI2LXa7e9wf7+GBHLIuKilNKUlNJBEfHvYuBUKePQaK/ZiFgYEdMj4qjBP/8jBj6S+sWaPzIN14CajcH9TI6B172JKaXJKaUJ9X9qmkzN0jQNqNnNA4ens1NKe6WU3h4Dv2zdUP+nboYx30xFRFRV9e2I+FpE/IeIeDoinoiIv4+BTnx3P4qIxyPiqYh4MCL+Zbfbz4mIxwZPmX4pImYP5ofFwEV222LgtwfXVlW1KiIipXRdSum6PWzx7weP+z+Dx/5DRNywh/szxo3mmh38rdWW1/7EwEdVXzaBcnwbzTU76PoYqNWzIuIbg/99ztB+SsYSNUvTjOaarapqa0R8NiIujIHrr9ZHxO8iYsx/D2WqKp8oAwAAGKpxcWYKAABguGmmAAAAatBMAQAA1KCZAgAAqGGPX/yVUjKdgo5UVZW6uZ6apVPdrtkIdUvnPNfSNGqWpmlVs85MAQAA1KCZAgAAqEEzBQAAUINmCgAAoIY9DqAYDxYvXpxlfX19WTZr1qwsW7169QjsCAAAaAJnpgAAAGrQTAEAANSgmQIAAKhBMwUAAFBDqqrWXwg9Hr4tek8//+uVhk2UhlLwl3zDOU3T7ZqNULd0znMtTaNmaZpWNevMFAAAQA2aKQAAgBo0UwAAADVopgAAAGrQTAEAANQwsdcb6KYTTzyx9rFr1qwZvo0AAACN58wUAABADZopAACAGjRTAAAANWimAAAAajCAAgBgFFm8eHGW9fX1ZVlKqQu7AfbEmSkAAIAaNFMAAAA1aKYAAABq0EwBAADUkKqqan1jSq1vbKA9/axvxEWe9VRV1dV/uLFWs3Rft2s2Qt32wrHHHptlK1euLN530qRJWXbxxRdnWX9/f+cbq8lz7djS7vuVJUuWFPPSAIvRRs3mZs6cWcznzZuXZR//+MezbP/998+y0vvXdevWZdnXv/714tr33ntvMR+PWtWsM1MAAAA1aKYAAABq0EwBAADUoJkCAACoYWKvNwAMeNOb3pRll156aZb19fVl2fbt27PsyiuvzLJVq1YV1y5dTP/JT34yy0oXsr7yyitZdsUVVxTXufzyy7Nsx44dxfvCcCkNkLjzzjuzbO+99y4e/+qrr2bZKaeckmW9HEBBczVhWATD76tf/WqWLVq0qHjf5557LsvWrFmTZdu2bcuyt7/97Vn20Y9+NMvuueee4tqzZ8/OsmXLlhXvO145MwUAAFCDZgoAAKAGzRQAAEANmikAAIAaDKCAUeKss87KstI3kldV/iXupQvs58+fn2Xve9/7imu/+OKLWfb4449n2cEHH5xlL730UpY98cQTxXV27dpVzGEkvfe9782yfffdt6PH/P3vf9/R8TAcDK9ohkMPPTTLSgOZvve97xWPX7hwYZb96U9/amvt0uCoGTNmZNnnPve54vE//OEPs2yvvfJzMTfffHNb+xmLnJkCAACoQTMFAABQg2YKAACgBs0UAABADWN2AEUnF2WuXr162PYB7Vq+fHmWXXXVVVm2zz77tPV4pQEU3//+94v33W+//bLs+eefb2ud0kW0rdaBkXb00Udn2c9//vNhX6d0UTbUccIJJ/R6C4ywt7zlLVm29957Z9ktt9xSPL7dYRMlpaFV69evbyuLKA/PmDt3bpYZQAEAAMCQaKYAAABq0EwBAADUoJkCAACoQTMFAABQw5id5tfJdJwlS5YM406gPZdeemmWvfWtb23r2G3btmXZL3/5y7bXnjNnTtv33d26detqHwudmDRpUpYtWrQoy97xjnd0tM4dd9yRZQ8//HBHj8n4tGrVqiw78cQT2zrWe5PmKk3KW7lyZZb19/cXj//Sl76UZZs3b+58Y2349a9/nWWnnXZaV9ZuCmemAAAAatBMAQAA1KCZAgAAqEEzBQAAUEOqqqr1jSm1vnGUaHXhZukiz3allGofy1+qqqqr/5hNqNlWNm3alGXTp0/Psp07d2bZqaeemmV33XVXlr3tbW8rrl36/+Xwww/Psg0bNmTZzJkzs2z79u3FdZqg2zUb0ey67YaJE8uzki677LIsu/jii2uvs3bt2mJeuth669attdcZCZ5rm2FP77neyFh7bzLea/aUU07Jsuuuu65430ceeSTLFixYkGX3339/5xvbzSc+8YksKz33PvTQQ1n2wQ9+MMuWLl1aXOfGG2+ssbvualWzzkwBAADUoJkCAACoQTMFAABQg2YKAACghvJVvQ2yevXqXm8BhuSAAw4o5lOmTGnr+NI3pJeGTZRMnTq1mJeGTZQsWbIky5o8bIJmmDVrVjHvZNhEyXe+851iPtqGTTD6tRqOBa9ZsWJFlpWGSUVE/OIXv8iyX/3qV1l23nnnZVm7gx322WefYl4awHPMMcdkWem9TWlQxU9/+tO29tMkzkwBAADUoJkCAACoQTMFAABQg2YKAACghsYPoBgJq1atyrLShfeGX1DHnDlzinmr4RC727BhQ+21zznnnNrHRkSsXbu2o+PhjbznPe/Jsuuvv76jx9y1a1eWffazn82yO+64o6N1GJ9KwyZK7yOGovSeg7Fv48aNxfy4447LsqVLl2bZddddl2XTpk3Lsm9/+9tZdtVVVxXX/uIXv5hlN910U5ZdcsklWfbkk08WH3OscWYKAACgBs0UAABADZopAACAGjRTAAAANaSqqlrfmFLrG0e5xYsXZ1lfX1/3NzJEQ7notDQAY7QNxaiqKnVzvSbU7B/+8IdiXvr28E2bNmXZjBkzsmznzp1trX311VcX86985SttHT99+vQse+GFF7Js9uzZxePPPffcLNu8eXOWLVy4MMsefPDBdrbYsW7XbEQz6nYklIZNrFixIssOPvjgjta59957s+zYY4/t6DFHG8+1vVMaNlEaStFK6XV71qxZHeyoGdRsZyZPnpxl5513Xpb19/dn2dNPP51lBx54YHGd0uv5smXL2tnimNOqZp2ZAgAAqEEzBQAAUINmCgAAoAbNFAAAQA2NGkBRuqCz1cCFkfhG8qYqDbUoDegYCS4wze3YsaOYT5w4Mcvmzp2bZddff31b67zzne/Msrvuuqt439JgiV4q1exQhrN0wgCK7pk3b16WLV26tKPHvPvuu7Ps85//fJaVLsBuMs+1vbOn91Ht6OVrdC+p2eE3adKkLCsN4PnABz6QZVu2bCk+5vHHH59lDz/8cI3dNZ8BFAAAAMNIMwUAAFCDZgoAAKAGzRQAAEANmikAAIAa8vFho0Rp8l5pQl9K5WEwpSl/s2bNamudsaavr6+t+42H6UHddt5552XZhAkT2j7+1FNPzbIjjjgiy4488sgs+/CHP5xlb37zm9teeyT87ne/y7KNGzdm2bJly7qxHbpo6tSpWXbuued29Jh//OMfs+xb3/pWlo21yX30TqeT+0rvTbz2MlwWLVqUZVOmTMmys88+O8sOOuig4mNu2rQpy+68884sK01nHS9T/5yZAgAAqEEzBQAAUINmCgAAoAbNFAAAQA1pTxdTppQ6u9KyA+1e5Fm6mDMiYsmSJW3ftx2l4RedPF5E5xeydqI0jKPTn6ekqqryhJAR0suaLV3ked9992XZ9OnTu7Gdjj366KNZtnz58iy7/fbbs+zZZ58tPmZp2MTOnTtr7G7kdLtmI3pbtyNhr73y39MtXbo0yy644IKO1lmzZk2WnXTSSR09ZlONp+fakVB6jS8NbyrdbyhaDc0aj9RsZz7zmc9k2a233pplV155ZZZ94xvfaHudM888M8v6+/uzbNu2bVl2zDHHZFlpcFBTtKpZZ6YAAABq0EwBAADUoJkCAACoQTMFAABQw8Reb6CV0oCEVatWZVmri0FLeWkoRUlpEEPp8YZyIWrpQtZuGe5hHJTt2rUry7Zv396Vtb/73e9mWekiz4suuqjtxywNi7jwwguHtjHGvNIF9V/4wheyrJNhEw899FAxnzNnTu3HhNfr9DV+d15jGWmHHHJIlpXeh/zsZz/raJ1ly5Zl2SOPPJJl69aty7Jbbrkly84666ziOi+//HKN3Y0OzkwBAADUoJkCAACoQTMFAABQg2YKAACghlRVrb8QerR9W/RQBlCMV6VhE4sXL+7+RgaN9284P+qoo7Ls5JNP7ugxf/zjH2fZc889l2Wf/vSns+y2225re53S0JTLL7+87eObqts1GzH66nYo3v3ud2fZ5s2bh3WNSy65pJj39/cP6zpNNt6fazu1p/dCb6Q0bKI0RIu/pGY7M3/+/CybO3dulk2fPr0b2ykOlrjhhhuybPny5cXjzzjjjGHf03BrVbPOTAEAANSgmQIAAKhBMwUAAFCDZgoAAKCGib3ewFCULugsDaWIGFuDKVp9k7oLXEe/9evXt5V1KqX8msjTTz+97eN37NiRZa0uEmV8+tCHPlTMV65cOazrLFiwIMuuueaaYV2D8a3V+4a6SoOfoBe2bNnSs7VvvvnmLNt3332z7Nprry0eP3v27Cy76aabOt9YFzgzBQAAUINmCgAAoAbNFAAAQA2aKQAAgBo0UwAAADWkqqpa35hS6xsbqDThr5T19fVl2UhM6ylN6Ws1ua+pqqrKx8yNoLFWs+2aMWNGlj3wwANtH79x48Yse//739/Rnpqq2zUb0Yy6nTlzZjG/5557hnWdadOmZdlTTz01rGuMRZ5r27en9z1vpDRFd6y9bneLmu3M/Pnzs+yyyy7LstJr+aOPPjoie9rd5MmTs2zDhg3F+/75z3/OstH2PqRVzTozBQAAUINmCgAAoAbNFAAAQA2aKQAAgBom9noD3dTuwIfFixeP+F5gOH3sYx/r6Pj+/v5h2glj1Ze//OVhf8wrrrgiy55++ulhX4fxqdPX8vEwJIrmuu2227Js3rx5WXbhhRdm2QUXXDAie9rd9u3bs2z58uXF+55xxhkjvZ0R48wUAABADZopAACAGjRTAAAANWimAAAAahhXAyhgrDrssMPaut8zzzxTzEfbRdXHH398lv3mN7/Jspdffrkb2yEi3vWud3V0/E9+8pMsW7hwYZa9+uqrHa0DdZSeA2fNmtX9jUCbHnvssSz7wQ9+kGWl59n7778/y2688cZh2dfrTZ06NctOOumk4n137Ngx7Ot3izNTAAAANWimAAAAatBMAQAA1KCZAgAAqMEAChgDjj322Lbu98QTTwwp75W1a9f2egt0oKqqLFu/fn2WGTbBSGo1WKevry/L1qxZM8K7gZF3ww03ZNnhhx+eZd/85jez7CMf+UjxMe++++4sO/DAA7OsNLDl0EMPzbL999+/uM6nPvWpYt4EzkwBAADUoJkCAACoQTMFAABQg2YKAACghlS6UPj/35hS6xuhDVVVpW6uN15r9re//W2WHXHEEVn2wAMPFI8/+uijh31PTdXtmo1oRt22upj/uOOOy7KtW7dmWauLjhkenmtpGjXbHRMmTMiy888/P8vmz59fPH7atGltrfPss89m2a233ppl11xzTfH4TZs2tbVOL7WqWWemAAAAatBMAQAA1KCZAgAAqEEzBQAAUIMBFIwoF5h2R39/f5YtWLAgy0oXg0ZEnHnmmcO+p6YygIIm8lxL06hZmsYACgAAgGGkmQIAAKhBMwUAAFCDZgoAAKAGzRQAAEANpvkxokzroWlM86OJPNfSNGqWpjHNDwAAYBhppgAAAGrQTAEAANSgmQIAAKhBMwUAAFCDZgoAAKAGzRQAAEANmikAAIAaNFMAAAA1pKryhdAAAABD5cwUAABADZopAACAGjRTAAAANWimAAAAatBMAQAA1KCZAgAAqOH/AQkrJNYGLjKfAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ],
      "metadata": {
        "id": "JAPDzW0wxhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "# load training and test data into data loaders, shuffle and divide into batches\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(dataset=train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset=test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"{len(train_loader)} batches for a total of {len(train_data)} samples in the training data.\")\n",
        "print(f\"{len(test_loader)} batches for a total of {len(test_data)} samples in the test data.\")"
      ],
      "metadata": {
        "id": "ALA6MPcFbJXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509b0bb4-c504-43fc-8ce7-9e751f28c278"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875 batches for a total of 60000 samples in the training data.\n",
            "313 batches for a total of 10000 samples in the test data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ],
      "metadata": {
        "id": "bCCVfXk5xjYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=10*7*7, out_features=10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc(self.block2(self.block1(x)))"
      ],
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = TinyVGG().to(device)"
      ],
      "metadata": {
        "id": "o69Pt3cLJeY7"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "summary(model2, (1, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpv_w1QALCVc",
        "outputId": "9fd0e5be-952c-43e3-8d77-ed37008e2990"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.7.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TinyVGG                                  [1, 10]                   --\n",
              "├─Sequential: 1-1                        [1, 10, 14, 14]           --\n",
              "│    └─Conv2d: 2-1                       [1, 10, 28, 28]           100\n",
              "│    └─ReLU: 2-2                         [1, 10, 28, 28]           --\n",
              "│    └─Conv2d: 2-3                       [1, 10, 28, 28]           910\n",
              "│    └─ReLU: 2-4                         [1, 10, 28, 28]           --\n",
              "│    └─MaxPool2d: 2-5                    [1, 10, 14, 14]           --\n",
              "├─Sequential: 1-2                        [1, 10, 7, 7]             --\n",
              "│    └─Conv2d: 2-6                       [1, 10, 14, 14]           910\n",
              "│    └─ReLU: 2-7                         [1, 10, 14, 14]           --\n",
              "│    └─Conv2d: 2-8                       [1, 10, 14, 14]           910\n",
              "│    └─ReLU: 2-9                         [1, 10, 14, 14]           --\n",
              "│    └─MaxPool2d: 2-10                   [1, 10, 7, 7]             --\n",
              "├─Sequential: 1-3                        [1, 10]                   --\n",
              "│    └─Flatten: 2-11                     [1, 490]                  --\n",
              "│    └─Linear: 2-12                      [1, 10]                   4,910\n",
              "==========================================================================================\n",
              "Total params: 7,740\n",
              "Trainable params: 7,740\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.15\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.16\n",
              "Params size (MB): 0.03\n",
              "Estimated Total Size (MB): 0.19\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ],
      "metadata": {
        "id": "sf_3zUr7xlhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# set up accuracy metric\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "accuracy_fn = Accuracy(task='multiclass', num_classes=10).to(device)\n",
        "\n",
        "# set up optimizer\n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a3w_eB9Ni9X",
        "outputId": "c928575e-e686-4466-bb84-2bd8b374b635"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: nn.Module, dataloader: DataLoader, loss_func: nn.Module, optimizer: torch.optim.Optimizer, device: torch.device = device):\n",
        "\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    logits = model(X)\n",
        "\n",
        "    # calculate loss\n",
        "    loss = loss_func(logits, y)\n",
        "    train_loss += loss\n",
        "\n",
        "    # calculate accuracy\n",
        "    labels = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "    acc = accuracy_fn(labels, y)\n",
        "    train_acc += acc\n",
        "\n",
        "    # zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient descent\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(dataloader)\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "    "
      ],
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: nn.Module, dataloader: DataLoader, loss_func: torch.nn, device: torch.device = device):\n",
        "  test_loss = 0.0\n",
        "  test_acc = 0.0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # prediction\n",
        "      logits = model(X)\n",
        "\n",
        "      # calculate and store loss\n",
        "      loss = loss_func(logits, y)\n",
        "      test_loss += loss\n",
        "\n",
        "      # calculate and store accuracy\n",
        "      labels = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "      acc = accuracy_fn(labels, y)\n",
        "      test_acc += acc\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    test_acc /= len(dataloader)\n",
        "\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "XtkvicqXPX3C"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "def train(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, loss_func: nn.Module, optimizer: torch.optim.Optimizer, device: torch.device = device):\n",
        "  results = {\n",
        "      'train_loss':[],\n",
        "      'train_acc':[],\n",
        "      'test_loss':[],\n",
        "      'test_acc':[]\n",
        "  }\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_step(model, train_loader, loss_func, optimizer, device)\n",
        "    test_loss, test_acc = test_step(model, test_loader, loss_func, device)\n",
        "\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc'].append(train_acc)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc'].append(test_acc)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "BwHrARm_Q2jn"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2_results = train(model2, train_loader, test_loader, loss_fn, optimizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBVKU1yvSejX",
        "outputId": "ce4bbd22-612c-4889-dc0a-5f12f5c986bd"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Train Loss: 0.66814 | Train Acc: 0.78% | Test Loss: 0.28650 | Test Acc: 0.92%\n",
            "Epoch: 2 | Train Loss: 0.23996 | Train Acc: 0.93% | Test Loss: 0.17749 | Test Acc: 0.95%\n",
            "Epoch: 3 | Train Loss: 0.16963 | Train Acc: 0.95% | Test Loss: 0.13000 | Test Acc: 0.96%\n",
            "Epoch: 4 | Train Loss: 0.13320 | Train Acc: 0.96% | Test Loss: 0.10519 | Test Acc: 0.97%\n",
            "Epoch: 5 | Train Loss: 0.11319 | Train Acc: 0.97% | Test Loss: 0.10392 | Test Acc: 0.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ],
      "metadata": {
        "id": "w1CsHhPpxp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: nn.Module, data: list, device: torch.device = device):\n",
        "  # store the probabilities of labels for each sample in the list\n",
        "  probabilities = []\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for sample in data:\n",
        "      # add a single batch dimension to the data and allocate the chosen device\n",
        "      sample = torch.unsqueeze(sample, dim=0).to(device)\n",
        "      # pred logits\n",
        "      preds = model(sample)\n",
        "      # calculate probability distribution over classes\n",
        "      probs = torch.softmax(preds.squeeze(), dim=0)\n",
        "      # store the distribution\n",
        "      probabilities.append(probs.cpu)\n",
        "\n",
        "  return torch.stack(probabilities) # stack all distributions into single tensor\n"
      ],
      "metadata": {
        "id": "ay3fRhmSc1PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = []\n",
        "test_labels = []\n",
        "# select 5 random image-labels pairs from the test data\n",
        "for sample, label in random.sample(list(test_data), k=N_SAMPLES):\n",
        "  test_samples.append(sample)\n",
        "  test_labels.append(label)"
      ],
      "metadata": {
        "id": "oXIsUYcthFR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate probabilites for all 5 random samples\n",
        "pred_probabilities = make_predictions(model2, test_samples, device)\n",
        "print(pred_probabilities)"
      ],
      "metadata": {
        "id": "ev7NJhgxhaWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of indexes with the highest probability from each distribution\n",
        "pred_classes = pred_probabilities.argmax(dim=1)"
      ],
      "metadata": {
        "id": "k_dcSKjZhq-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "N_SAMPLES = 5\n",
        "# select 5 random indexes from the training data\n",
        "random_idx = random.sample(range(len(test_data)), k=N_SAMPLES)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 3))\n",
        "# plot the image and label from the training data at each random index from the list above\n",
        "for i, rand_idx in enumerate(random_idx):\n",
        "  image, label = test_data[rand_idx][0], test_data[rand_idx][1]\n",
        "\n",
        "  image = image.to(device)\n",
        "  \n",
        "  model2.eval()\n",
        "  with torch.inference_mode():\n",
        "    pred_logit = model2(image)\n",
        "    pred_label = test_data.classes[torch.argmax(torch.softmax(pred_logit, dim=1), dim=1)]\n",
        "\n",
        "    if test_data.classes[label] == pred_label:\n",
        "      colour = 'g'\n",
        "    else:\n",
        "      colour = 'r'\n",
        "\n",
        "    fig.add_subplot(1, 5, i+1)\n",
        "    plt.imshow(image.squeeze(dim=0), cmap='gray')\n",
        "    plt.title(f\"Predicted Class: {pred_label}\", c=colour)\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "_YGgZvSobNxu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "e83f4d10-b700-44f1-de71-26617e6c8bdd"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-0c96c2196889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpred_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_logit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-94673841f15e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x49 and 490x10)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x216 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ],
      "metadata": {
        "id": "qQwzqlBWxrpG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ],
      "metadata": {
        "id": "lj6bDhoWxt2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.randn(1, 3, 64, 64)\n",
        "print(f\"Original shape: {X.shape}\")\n",
        "\n",
        "# convolution layers using increasing sized kernels\n",
        "conv_kernel_3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "conv_kernel_5 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=1, padding=1)\n",
        "conv_kernel_7 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=7, stride=1, padding=1)\n",
        "\n",
        "print(f\"3x3 kernel output shape: {conv_kernel_3(X).shape}\")\n",
        "print(f\"5x5 kernel output shape: {conv_kernel_5(X).shape}\")\n",
        "print(f\"7x7 kernel output shape: {conv_kernel_7(X).shape}\")"
      ],
      "metadata": {
        "id": "leCTsqtSbR5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308c76d9-4516-4964-fe55-5e04aa049ef5"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: torch.Size([1, 3, 64, 64])\n",
            "3x3 kernel output shape: torch.Size([1, 3, 64, 64])\n",
            "5x5 kernel output shape: torch.Size([1, 3, 62, 62])\n",
            "7x7 kernel output shape: torch.Size([1, 3, 60, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the kernel size increases, the output of the convolution becomes smaller. As the kernel size decreasess"
      ],
      "metadata": {
        "id": "W7qjFQpoaD0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ],
      "metadata": {
        "id": "VHS20cNTxwSi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "execution_count": 113,
      "outputs": []
    }
  ]
}