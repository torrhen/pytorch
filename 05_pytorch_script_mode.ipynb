{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9ZW6ciTwjnRyOT7aempZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/pytorch/blob/main/05_pytorch_script_mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bhFq16J57VR8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# create new folder for stroing modular files\n",
        "os.makedirs('script_mode', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile # file extracting\n",
        "from pathlib import Path # file management\n",
        "import requests # HTTP requests\n",
        "\n",
        "# store file paths\n",
        "data_path = Path('data/')\n",
        "image_path = data_path / 'pizza_steak_sushi'\n",
        "\n",
        "# create the image pth folder if it does not exist\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} already exists.\")\n",
        "else:\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# download image zip file from GitHub\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", 'wb') as f:\n",
        "  response = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  f.write(response.content)\n",
        "\n",
        "# extract images from image zip file\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", 'r') as z:\n",
        "  z.extractall(image_path)\n",
        "\n",
        "# delete download image zip file - no longer required\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLr1Uy6NDxpq",
        "outputId": "74d80eca-892c-4b8d-af52-845c883e2a85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qlnQxHhPJx5A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the following code to a new file data.py\n",
        "%%writefile script_mode/data.py\n",
        "\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "def create_dataloaders(train_dir: str, test_dir: str, transform: transforms.Compose, batch_size: int, num_workers: int = os.cpu_count()):\n",
        "  # create datasets from image file folder and apply transform\n",
        "  train_data = datasets.ImageFolder(root=train_dir, transform=transform, target_transform=None)\n",
        "  test_data = datasets.ImageFolder(root=test_dir, transform=transform, target_transform=None)\n",
        "\n",
        "  # divide the datasets into batches and shuffle if applicable\n",
        "  train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  return train_loader, test_loader, train_data.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bukpix2bJ77G",
        "outputId": "03a1e544-5b41-420a-b1df-6f830fb15591"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting script_mode/data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write the following code to a new file model.py\n",
        "%%writefile script_mode/model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_size, hidden_units, output_size):\n",
        "    super().__init__()\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_size, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*13*13, out_features=output_size)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc(self.block2(self.block1(x)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8LW9xF3TxT8",
        "outputId": "bfde5a10-9e18-45b2-9fc8-6bfb7a9c3364"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting script_mode/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script_mode/train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "def train_step(model: nn.Module,\n",
        "               dataloader: DataLoader,\n",
        "               loss_fn: nn.Module,\n",
        "               accuracy_fn: Accuracy,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  \n",
        "  train_loss, train_acc = 0.0, 0.0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # allocate to device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # forward pass\n",
        "    logits = model(X)\n",
        "    labels = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "    # calculate loss\n",
        "    loss = loss_fn(logits, y)\n",
        "    train_loss += loss\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_fn(labels, y)\n",
        "    train_acc += acc\n",
        "\n",
        "    # stop accumulation of gradients\n",
        "    optimizer.zero_grad()\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "    # gradient descent update\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(dataloader)\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: nn.Module,\n",
        "              dataloader: DataLoader,\n",
        "              loss_fn: nn.Module,\n",
        "              accuracy_fn: Accuracy,\n",
        "              device: torch.device):\n",
        "  \n",
        "  test_loss, test_acc = 0.0, 0.0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # allocate to device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # forward pass\n",
        "      logits = model(X)\n",
        "      labels = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "      # calculate loss\n",
        "      loss = loss_fn(logits, y)\n",
        "      test_loss += loss\n",
        "      # calculate accuracy\n",
        "      acc = accuracy_fn(labels, y)\n",
        "      test_acc += acc\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    test_acc /= len(dataloader)\n",
        "\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          train_loader: DataLoader,\n",
        "          test_loader: DataLoader,\n",
        "          loss_fn: nn.Module,\n",
        "          accuracy_fn: Accuracy,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          device:  torch.device,\n",
        "          epochs: int = 5):\n",
        "  \n",
        "  results = {'train_loss':[],\n",
        "             'train_acc':[],\n",
        "             'test_loss':[],\n",
        "             'test_acc':[]}\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # train the model on the training data\n",
        "    train_loss, train_acc = train_step(model, train_loader, loss_fn, accuracy_fn, optimizer, device)\n",
        "    # test the model on the test data\n",
        "    test_loss, test_acc = test_step(model, test_loader, loss_fn, accuracy_fn, device)\n",
        "\n",
        "    # print results for each epoch\n",
        "    print(f\"Epoch: {epoch+1} | Train Loss: {train_loss:.5f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.5f} | Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "    # store the results for each epoch\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc'].append(train_acc)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc'].append(test_acc)\n",
        "  \n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QryKOJP26HyB",
        "outputId": "432cbb71-cec1-46a0-cd83-508f5226711d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting script_mode/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script_mode/utils.py\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_folder: str,\n",
        "               model_name: str):\n",
        "  \n",
        "  # store file path to save model\n",
        "  target_folder_path = Path(target_folder)\n",
        "  # create folder where model will be saved\n",
        "  target_folder_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  save_path = target_folder_path / model_name\n",
        "  # save model to save path\n",
        "  torch.save(obj=model.state_dict(), f=save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKzHA7-jVmTp",
        "outputId": "12af79c9-2e33-4aea-bf1d-77d9b8530dbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting script_mode/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script_mode/main.py\n",
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import data, model, train, utils\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# training and test folders\n",
        "train_folder = \"../data/pizza_steak_sushi/train\"\n",
        "test_folder = \"../data/pizza_steak_sushi/test\"\n",
        "\n",
        "# setup transform to be applied when creating data loaders\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "# create data loaders for training and test data\n",
        "train_loader, test_loader, classes = data.create_dataloaders(train_folder, test_folder, transform, BATCH_SIZE)\n",
        "\n",
        "# create model\n",
        "torch.manual_seed(42)\n",
        "model1 = model.TinyVGG(3, 10, 3).to(device) # allocate to device\n",
        "\n",
        "# create accuracy function\n",
        "accuracy_fn = Accuracy(task='multiclass', num_classes=3)\n",
        "\n",
        "# multiclass cross entropy loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(params=model1.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train the model\n",
        "model1_results = train.train(model1, train_loader, test_loader, loss_fn, accuracy_fn, optimizer, device, N_EPOCHS)\n",
        "\n",
        "# save the model\n",
        "utils.save_model(model1, 'models', 'model1.pth')"
      ],
      "metadata": {
        "id": "V7f80p5-DNeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c1c06c-f40d-47a7-9750-a70fc22eb702"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting script_mode/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# navigate to the folder containing main.py\n",
        "%cd script_mode\n",
        "!python main.py # run from terminal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyX5DUlJchQt",
        "outputId": "6a1ca1ce-585f-419f-d0d2-2eaa15321b83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/script_mode\n",
            "Epoch: 1 | Train Loss: 1.10634 | Train Accuracy: 0.30 | Test Loss: 1.09832 | Test Accuracy: 0.29\n",
            "Epoch: 2 | Train Loss: 1.09951 | Train Accuracy: 0.33 | Test Loss: 1.06984 | Test Accuracy: 0.54\n",
            "Epoch: 3 | Train Loss: 1.08625 | Train Accuracy: 0.49 | Test Loss: 1.07999 | Test Accuracy: 0.52\n",
            "Epoch: 4 | Train Loss: 1.08246 | Train Accuracy: 0.41 | Test Loss: 1.05990 | Test Accuracy: 0.57\n",
            "Epoch: 5 | Train Loss: 1.06301 | Train Accuracy: 0.41 | Test Loss: 1.06117 | Test Accuracy: 0.55\n"
          ]
        }
      ]
    }
  ]
}